{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "Algorithms that can compute optimal policies (solve RL problems) given a perfect model of the environment as an MDP. Computationally, we require a finite MDP and a small enough problem that we can actually solve it in a reasonable time; this is often called the \"tabular\" case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bellman optimality equations are\n",
    "$$ v_*\\left(s\\right) = \\max_a \\sum_{s',r} p\\left(s',r\\middle|s,a\\right) \\left[r + \\gamma v_*\\left(s'\\right)\\right] $$\n",
    "and\n",
    "$$ q_*\\left(s,a\\right) = \\sum_{s',r} p\\left(s',r\\middle|s,a\\right) \\left[r + \\gamma \\max_{a'} q_*\\left(s', a'\\right)\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL DP algorithms are derived by converting these into update formulas that can be used to iteratively improve an estimate of the value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation / Prediction\n",
    "\n",
    "Given a policy $\\pi$, we would like to compute $v_{\\pi}\\left(s\\right) \\forall s \\in \\mathcal{S}$. We have seen earlier that this is a linear system of equations, but it is often tedious to set up and solve the system. The **iterative policy evaluation algorithm** estimates it via an update formula:\n",
    "\n",
    "$$ \\begin{align*} v_{k+1}\\left(s\\right) & = \\mathbb{E}_{\\pi}\\left[R_{t+1} + \\gamma v_k \\left(S_{t+1}\\right) \\middle| S_t = s \\right] \\\\ & = \\sum_a \\pi\\left(a\\middle|s\\right) \\sum_{s',r} p\\left(s',r\\middle| s,a\\right) \\left[r + \\gamma v_k\\left(s'\\right) \\right]\\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks nearly identical to the Bellman equation for $v_{\\pi}$, but the crucial difference is that $v_{k+1}$ on the left-hand side and $v_k$ on the right are not the same function; the former is an incremental update of the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will set up the environment/dynamics of the chapter 4 gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: up, 1: right, 2: down, 3: left\n",
    "def dynamics(s, a):\n",
    "    \"\"\"Dynamics for the episodic gridworld environment.\n",
    "    Given (s,a), return a list of (s', r, p), possible outcomes.\n",
    "    \"\"\"\n",
    "    if s == 0 or s == 15: # terminal states\n",
    "        return [(0, 0, 1.0)]\n",
    "    # else we are not in the terminal states\n",
    "\n",
    "    if a == 0: # moving up\n",
    "        if s < 4: # hit top boundary, stay in place\n",
    "            return [\n",
    "                (s, -1, 1.0),\n",
    "            ]\n",
    "        else: # we are not at the top row\n",
    "            return [\n",
    "                (s - 4, -1, 1.0)\n",
    "            ]\n",
    "    elif a == 1: # moving right\n",
    "        if (s+1) % 4 == 0: # hit right boundary\n",
    "            return [\n",
    "                (s, -1, 1.0)\n",
    "            ]\n",
    "        else: # we are not in the right row\n",
    "            return [\n",
    "                (s+1, -1, 1.0),\n",
    "                ]\n",
    "        \n",
    "    elif a == 2: # moving down\n",
    "        if s >= 12: # hit bottom boundary\n",
    "            return [\n",
    "                (s, -1, 1.0)\n",
    "                ]\n",
    "        else: # we are not in the bottom row\n",
    "            return [\n",
    "                (s + 4, -1, 1.0)\n",
    "                ]\n",
    "        \n",
    "    elif a == 3: # moving left\n",
    "        if s % 4 == 0: # hit left boundary\n",
    "            return [\n",
    "                (s, -1, 1.0)\n",
    "            ]\n",
    "        else: # not in leftmost row\n",
    "            return [\n",
    "                (s - 1, -1, 1.0)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def plot_gridworld_policy(policy: Callable, ax = None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    ax.set(xlim=[-0.5,3.5], ylim=[-0.5,3.5])\n",
    "    ax.set_xticks(np.arange(-0.5, 3.5, 1))\n",
    "    ax.set_yticks(np.arange(-0.5, 3.5, 1))\n",
    "    ax.xaxis.set_ticklabels([])\n",
    "    ax.yaxis.set_ticklabels([])\n",
    "    ax.grid()\n",
    "\n",
    "    def state_to_txt_coord(s: int):\n",
    "        y_coord = 4 - ( s // 4) - 1\n",
    "        x_coord = s % 4\n",
    "        return x_coord, y_coord\n",
    "\n",
    "    arrows = {0: [0,1], 1: [1, 0], 2: [0, -1], 3: [-1, 0]}\n",
    "    scale = 0.40\n",
    "    eps = 1e-5\n",
    "    for s in range(16):\n",
    "        x_t, y_t = state_to_txt_coord(s)\n",
    "        for a, p_a_s in policy(s):\n",
    "            arrow = np.array(arrows[a])\n",
    "            arrow = scale * p_a_s * arrow\n",
    "            if any(abs(arrow) > eps):\n",
    "                ax.arrow(\n",
    "                    x_t,\n",
    "                    y_t,\n",
    "                    arrow[0],\n",
    "                    arrow[1],\n",
    "                    head_width=0.05,\n",
    "                    head_length=0.1,\n",
    "                    fc='k',\n",
    "                    ec='k',\n",
    "                    )\n",
    "    ax.set_aspect('equal')\n",
    "    # plt.show()\n",
    "\n",
    "def plot_v(v: np.ndarray, ax = None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    ax.set(xlim=[-0.5,3.5], ylim=[-0.5,3.5])\n",
    "    ax.set_xticks(np.arange(-0.5, 3.5, 1))\n",
    "    ax.set_yticks(np.arange(-0.5, 3.5, 1))\n",
    "    ax.xaxis.set_ticklabels([])\n",
    "    ax.yaxis.set_ticklabels([])\n",
    "    ax.grid()\n",
    "\n",
    "    \n",
    "    def state_to_txt_coord(s: int):\n",
    "        y_coord = 4 - ( s // 4) - 1\n",
    "        x_coord = s % 4\n",
    "        return x_coord, y_coord\n",
    "\n",
    "    for s in range(len(v)):\n",
    "        x_t, y_t = state_to_txt_coord(s)\n",
    "        ax.text(x=x_t, y=y_t, s=f\"{v[s]:.1f}\")\n",
    "    ax.set_aspect('equal')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eqr_policy(s):\n",
    "    \"\"\"Equiprobable random policy.\n",
    "    Returns a list of (action_index, probability) tuples.\n",
    "    Note that s is unused since this is random policy\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (0, 0.25),\n",
    "        (1, 0.25),\n",
    "        (2, 0.25),\n",
    "        (3, 0.25),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_km1 = np.zeros(16)\n",
    "v_k = np.zeros(16)\n",
    "\n",
    "tol = 0.001\n",
    "max_iter = 200\n",
    "iter_ = 1\n",
    "\n",
    "while iter_ <= max_iter:\n",
    "    del_ = 0\n",
    "    for s in range(16):\n",
    "        for a, p_a_s in eqr_policy(s):\n",
    "            for sp, r, p_spr_sa in dynamics(s, a):\n",
    "                v_k[s] += p_a_s * p_spr_sa * (r + v_km1[sp])\n",
    "        del_ = np.max([del_, np.abs(v_k[s] - v_km1[s])])\n",
    "    v_km1 = v_k.copy()\n",
    "    v_k = np.zeros(16)\n",
    "\n",
    "    if del_ < tol:\n",
    "        print(f\"converged on iteration {iter_}\")\n",
    "        break\n",
    "\n",
    "    iter_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_v(v_km1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_q_from_v(v):\n",
    "    \"\"\"Given a state-value function v and the problem dynamics,\n",
    "    compute the corresponding action-value function.\n",
    "    \"\"\"\n",
    "    gamma = 1\n",
    "    v_f = v.flatten()\n",
    "    q = np.zeros((16, 4)) # states, actions\n",
    "    for s in range(16):\n",
    "        for a in range(4):\n",
    "            for sp, r, p_sp_r in dynamics(s, a):\n",
    "                q[s, a] += (r + gamma * v_f[sp]) * p_sp_r\n",
    "    return q\n",
    "\n",
    "\n",
    "def calc_greedy_policy_from_v(v):\n",
    "    \"\"\"Given a state-value function, calculate a greedy policy.\"\"\"\n",
    "    # get action-value function\n",
    "    q = calc_q_from_v(v)\n",
    "\n",
    "    def greedy_policy(s):\n",
    "        # Greedy policy assigns nonzero probability only to actions that\n",
    "        # give maximal q(s,a)\n",
    "        exp_return_a = [q[s,a] for a in range(4)]\n",
    "\n",
    "        # then take the max\n",
    "        opt_actions = np.where(exp_return_a == np.max(exp_return_a))[0]\n",
    "        p_ = 1/len(opt_actions)\n",
    "        return [(oa, p_) for oa in opt_actions]\n",
    "    return greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_greedy = calc_greedy_policy_from_v(v_km1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gridworld_policy(policy_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State is $\\left(s_1, s_2\\right) \\in \\left[0, 20\\right]^2$\n",
    "\n",
    "Actions are $a \\in \\left[-5, 5\\right]$\n",
    "\n",
    "A policy is $ \\pi\\left(a\\middle|s\\right) $\n",
    "\n",
    "The dynamics are given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{align*} p_1\\left(n_{req}\\right) = \\frac{3^n}{n!}\\exp^{-3} \\\\ p_2\\left(n_{req}\\right) = \\frac{4^n}{n!}\\exp^{-4} \\\\ p_1\\left(n_{ret}\\right) = \\frac{3^n}{n!}\\exp^{-3} \\\\ p_2\\left(n_{ret}\\right) = \\frac{2^n}{n!}\\exp^{-2}   \\end{align*} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "def _poisson_pdf(lambda_, n):\n",
    "    return (lambda_ ** n) * np.exp(-lambda_) / factorial(n)\n",
    "\n",
    "def p1_req(n):\n",
    "    \"\"\"Probability of n requests from location 1.\"\"\"\n",
    "    return _poisson_pdf(3, n)\n",
    "def p1_ret(n):\n",
    "    \"\"\"Probability of n returns to location 1.\"\"\"\n",
    "    return _poisson_pdf(3, n)\n",
    "def p2_req(n):\n",
    "    \"\"\"Probability of n requests from location 2.\"\"\"\n",
    "    return _poisson_pdf(4, n)\n",
    "def p2_ret(n):\n",
    "    \"\"\"Probability of n returns to location 2.\"\"\"\n",
    "    return _poisson_pdf(2, n)\n",
    "\n",
    "\n",
    "actions = zip(range(11), range(-5,6,1))\n",
    "def dynamics(s, a):\n",
    "    \"\"\"Return [(sp, r, p_spr_sa)]\"\"\"\n",
    "\n",
    "    # loop over every scenario\n",
    "    for n_req_1 in range(21):\n",
    "        for n_req_2 in range(21):\n",
    "            for n_ret_1 in range(21):\n",
    "                for n_ret_2 in range(21):\n",
    "\n",
    "                    # calculate the return (deterministic now)\n",
    "                    r = 10 * np.max([0, s[0] - n_req_1]) + 10 * np.max([0, s[1] - n_req_2]) - 2 * actions[a]\n",
    "\n",
    "                    # calculate the new state\n",
    "                    s1 = (s[0] - np.max([0, s[0] - n_req_1])) - actions[a] + n_ret_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.ones((20, 20))\n",
    "Z[:10, :10] = 10\n",
    "Z[:5, :] = 5\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(Z)\n",
    "plt.colorbar(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hard part here is calculating and storing the dynamics $p\\left(s',r\\middle|s,a\\right)$.\n",
    "\n",
    "$$ p\\left(s_1', s_2', r \\middle| s_1, s_2, a\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    s1: int\n",
    "    s2: int\n",
    "\n",
    "@dataclass\n",
    "class Action:\n",
    "    a: int\n",
    "\n",
    "states = dict(enumerate([ State(a,b) for a in range(0, 21) for b in range(0, 21) ]))\n",
    "actions = dict(enumerate([ Action(a) for a in range(-5, 6)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by computing $p\\left(s', r\\middle| s,a\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = states[6]\n",
    "a = actions[4]\n",
    "\n",
    "# need to iterate over every possibility\n",
    "\n",
    "if a.a > s.s1 or (a.a < 0 and -a.a > s.s2):\n",
    "     prob_ = 0.0\n",
    "else:\n",
    "    for n_req_1 in range(21):\n",
    "            p1_req_p = p1_req(n_req_1)\n",
    "            for n_req_2 in range(21):\n",
    "                p2_req_p = p2_req(n_req_2)\n",
    "                for n_ret_1 in range(21):\n",
    "                    p1_ret_p = p1_ret(n_ret_1)\n",
    "                    for n_ret_2 in range(21):\n",
    "                        p2_ret_p = p2_ret(n_ret_2)\n",
    "                        # calculate the return (deterministic now)\n",
    "                        r = 10 * np.max([0, s.s1 - n_req_1]) + 10 * np.max([0, s.s2 - n_req_2]) - 2 * a.a\n",
    "                        \n",
    "                        # calculate the new state\n",
    "                        s1 = (\n",
    "                             s.s1 - np.max([0, s.s1 - n_req_1]) # subtract demand\n",
    "                            - a.a \\ + n_ret_1\n",
    "                        )\n",
    "                        \n",
    "                        # get the probability of this occurring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import factorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transition tuple is $\\left(s, a, s', r, p\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = []\n",
    "states = [ (a, b) for a in range(21) for b in range(21) ]\n",
    "# states = [ (a, b) for a in range(21) for b in range(21) ]\n",
    "\n",
    "actions = dict(enumerate(range(-5, 6, 1)))\n",
    "\n",
    "\n",
    "class Poisson:\n",
    "    \"\"\"Helper for obtaining probability distributions.\"\"\"\n",
    "    @staticmethod\n",
    "    def _factorial(n_max):\n",
    "        \"\"\"Compute the factorials of an array of sequential integers.\"\"\"\n",
    "        n_fact = np.ones(n_max + 1, dtype=int)\n",
    "        for i in range(2, n_max + 1):\n",
    "            n_fact[i] = n_fact[i-1] * i\n",
    "        return n_fact\n",
    "    \n",
    "    def get(lambda_):\n",
    "        return (lambda_ ** np.arange(0,21,1)) * np.exp(-lambda_) / Poisson._factorial(20)\n",
    "    \n",
    "def action_valid(s, a):\n",
    "    \"\"\"Check that action a is valid in state s\"\"\"\n",
    "    if a > 0:\n",
    "        return a <= s[0] # valid if there are enough cars at loc A\n",
    "    elif a < 0:\n",
    "        return -a <= s[1] # valid if there are enough cars at loc B\n",
    "    else:\n",
    "        return True # a == 0\n",
    "\n",
    "def calc_new_state_and_reward(s, a, n_a_ordered, n_a_returned, n_b_ordered, n_b_returned):\n",
    "    \"\"\"Compute s' and r for this combination of state, action, and observed customer\n",
    "    behaviors.\n",
    "    \"\"\"\n",
    "    # Move cars\n",
    "    s_int = (s[0]-a, s[1]+a) # intermediate\n",
    "\n",
    "    # Rent out as many cars as possible; accept returned cars\n",
    "    # Clip total number of cars at 20\n",
    "    n_a_rented = min([n_a_ordered, s_int[0]])\n",
    "    n_b_rented = min([n_b_ordered, s_int[1]])\n",
    "    sp = (\n",
    "        np.max([s_int[0] - n_a_rented + n_a_returned, 20]),\n",
    "        np.max([s_int[1] - n_b_rented + n_b_returned, 20]),\n",
    "        )\n",
    "    \n",
    "    # Calculate the reward\n",
    "    r = -2 * np.abs(a) + 10 * (n_a_rented + n_b_rented)\n",
    "    return sp, r\n",
    "\n",
    "p_a_order = p_a_return = Poisson.get(3)\n",
    "p_b_order = Poisson.get(4)\n",
    "p_b_return = Poisson.get(2)\n",
    "\n",
    "# rather than construct a function like prob(s', r, s, a), which would be\n",
    "# very intensive to construct and evaluate, I will create a list of transition\n",
    "# tuples. Note that some future states and rewards (s', r, ...) will be repeated,\n",
    "# corresponding to different samples from the stochastic dynamics. This is OK\n",
    "# since we will be summing them anyway.\n",
    "for s in states:\n",
    "    for a_idx, a in actions.items():\n",
    "        if action_valid(s, a):\n",
    "            for n_a_ordered in range(2):\n",
    "                for n_a_returned in range(2):\n",
    "                    for n_b_ordered in range(2):\n",
    "                        for n_b_returned in range(2):\n",
    "            # for n_a_ordered in range(21):\n",
    "            #     for n_a_returned in range(21):\n",
    "            #         for n_b_ordered in range(21):\n",
    "            #             for n_b_returned in range(21):\n",
    "                            # calc probability of this happening (transition probability)\n",
    "                            prob_customers = p_a_order[n_a_ordered] * p_a_return[n_a_returned] * p_b_order[n_b_ordered] * p_b_return[n_b_returned]\n",
    "\n",
    "                            # calc new state and reward\n",
    "                            sp, r = calc_new_state_and_reward(s=s, a=a, n_a_ordered=n_a_ordered, n_a_returned=n_a_returned, n_b_ordered=n_b_ordered, n_b_returned=n_b_returned)\n",
    "\n",
    "                            # create transition tuple (s, a ; s', r, p)\n",
    "                            transitions.append((s, a, sp, r, prob_customers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    " v_{k+1}\\left(s\\right) & = \\sum_a \\pi\\left(a\\middle|s\\right) \\sum_{s',r} p\\left(s',r\\middle|s,a\\right) \\left(r + \\gamma v_{k}\\left(s'\\right) \\right) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poisson:\n",
    "    \"\"\"Helper for obtaining probability distributions.\"\"\"\n",
    "    @staticmethod\n",
    "    def _factorial(n_max):\n",
    "        \"\"\"Compute the factorials of an array of sequential integers.\"\"\"\n",
    "        n_fact = np.ones(n_max + 1, dtype=int)\n",
    "        for i in range(2, n_max + 1):\n",
    "            n_fact[i] = n_fact[i-1] * i\n",
    "        return n_fact\n",
    "    \n",
    "    def get(lambda_):\n",
    "        return (lambda_ ** np.arange(0,21,1)) * np.exp(-lambda_) / Poisson._factorial(20)\n",
    "    \n",
    "# probs_customers[n_a_ordered, n_a_returned, n_b_ordered, n_b_returned]\n",
    "p2 = Poisson.get(2)\n",
    "p3 = Poisson.get(3)\n",
    "p4 = Poisson.get(4)\n",
    "probs_customers = np.zeros((21, 21, 21, 21))\n",
    "for n_a_ordered in range(21):\n",
    "    for n_a_returned in range(21):\n",
    "        for n_b_ordered in range(21):\n",
    "            for n_b_returned in range(21):\n",
    "                probs_customers[n_a_ordered, n_a_returned, n_b_ordered, n_b_returned] = p3[n_a_ordered] * p3[n_a_returned] * p4[n_b_ordered] * p4[n_b_returned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (5, 6)\n",
    "a = 3\n",
    "\n",
    "def move_cars(s, a):\n",
    "    \"\"\"Allow impossible moves (these will be filtered out later).\"\"\"\n",
    "    return (s[0] - a, s[1] + a)\n",
    "\n",
    "\n",
    "def calculate_rewards(s_moved):\n",
    "    \"\"\"Calculate all possible rewards given a state and an action.\n",
    "    There will be a different rewards for each different scenario, i.e.,\n",
    "    combination of (n_a_ordered, n_a_returned, n_b_ordered, n_b_returned).\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "# estimate the value of a state and action pair\n",
    "s = (5, 6)\n",
    "a = 3\n",
    "\n",
    "s_moved = move_cars(s, a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update formula for policy evaluation is\n",
    "$$ v_{k+1}\\left(s\\right) = \\sum_{a} \\pi\\left(a\\middle| s\\right) \\sum_{s', r} p\\left(s', r \\middle| s,a\\right)\\left[ r + \\gamma v_k \\left(s'\\right)\\right] $$\n",
    "\n",
    "I am trying to rewrite this to make it simpler to compute. My issue now is that implementing the function $p\\left(s', r \\middle| s,a\\right)$ is incredible complicated. It is hard to represent as an array because:\n",
    "1. The states are actually tuples $\\left(s_1, s_2\\right)$, so the array would be six dimensional.\n",
    "2. Worse, the rewards are a priori unknown and so it's impossible to know how big the reward dimension is and how to index it (since they are real numbers).\n",
    "\n",
    "The first problem can be handled by standardizing an indexing scheme for the states. The second problem will be made irrelevant if I can remove $r$ as an index in these arrays.\n",
    "\n",
    "To start, let's look at the innermost (double) summation:\n",
    "$$ \\sum_{s', r} p\\left(s', r \\middle| s,a\\right)\\left[ r + \\gamma v_k \\left(s'\\right)\\right] $$\n",
    "\n",
    "I will split this out and try to manipulate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can \n",
    "\\begin{align*}\n",
    "    p\\left(s',r\\middle|s,a\\right) & = p\\left(s'\\middle|r,s,a\\right) p\\left(r\\middle|s,a\\right) \\\\\n",
    "    & = p\\left(s'\\middle|s,a\\right) p\\left(r\\middle|s,a\\right) \\\\\n",
    "\\end{align*}\n",
    "where the first equation comes from basic laws of conditional probability, and the second equation comes from conditional independence: $s'$ is conditionally independent of $r$ given $s,a$. Taking that inner sum above and substituting this factored form of the probability distribution, we get\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sum_{s', r} p\\left(s', r \\middle| s,a\\right)\\left[ r + \\gamma v_k \\left(s'\\right)\\right] & = \\sum_{s', r} r p\\left(s', r \\middle| s,a\\right)+ \\gamma \\sum_{s', r} v_k \\left(s'\\right)p\\left(s',r\\middle|s,a\\right) \\\\\n",
    "    & = \\sum_{s', r} r p\\left(r\\middle|s,a\\right) p\\left(s'\\middle| s,a\\right) + \\gamma \\sum_{s', r} v_k \\left(s'\\right)p\\left(r\\middle|s,a\\right)p\\left(s'\\middle|s,a\\right) \\\\\n",
    "    & = \\sum_{s'}  p\\left(s'\\middle| s,a\\right) \\sum_r r p\\left(r\\middle|s,a\\right) + \\gamma \\sum_{s'} v_k \\left(s'\\right)p\\left(s'\\middle|s,a\\right) \\sum_r p\\left(r\\middle|s,a\\right) \\\\\n",
    "    & = \\sum_r r p\\left(r\\middle|s,a\\right) + \\gamma \\sum_{s'} v_k \\left(s'\\right)p\\left(s'\\middle|s,a\\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "where the last equation follows from the definition of expected value and the law of total probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, notice that the sums in the last line are actually expected values under the reward distribution and the transition distribution, respectively:\n",
    "$$ \\sum_{s',r} p\\left(s',r\\middle|s,a\\right) \\left[r + \\gamma v_k\\left(s'\\right)\\right] = \\mathbb{E}\\left[ R_{t+1} \\middle| S_t=s, A_t=a \\right] + \\gamma \\mathbb{E}\\left[ v_k\\left(S_{t+1}\\right) \\middle| S_t = s, A_t = a\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this help us solve the issues we were facing? I believe so! We should be able to precompute both terms and store them in a matrix, each indexed by $\\left(s,a\\right)$.\n",
    "\n",
    "In the following, let a \"scenario\" be a particular instantiation of the random variables $\\left(n_o^A, n_r^A, n_o^B, n_r^B\\right)$, representing a possible combination of observed customer orders and returns. It is over these \"scenarios\" that the expectations are taken.\n",
    "\n",
    "For notational convenience, let $S = \\left\\{0,1,\\ldots,20\\right\\}^2$ be the state space and $A = \\left\\{-5,-4,\\ldots, 4, 5\\right\\}$ be the action space.\n",
    "\n",
    "#### The first term\n",
    "\n",
    "$$ \\mathbb{E}\\left[ R_{t+1} \\middle| S_t = s, A_t = a \\right] $$\n",
    "\n",
    "Given a state $s$ and an action $a$, we can store the expected reward function is a mapping $S \\times A \\rightarrow \\mathbb{R}$. Since each state is two-dimensional, this could easily be a three-dimensional array; but since tuples are hashable in Python, I will first create a mapping from each state tuple to an integer index, then use this integer index in the reward expectation matrix.\n",
    "\n",
    "I will precompute the expected reward for every $\\left(s,a\\right)$ by:\n",
    "1. Generating every possible scenario $\\left(n_o^A, n_r^A, n_o^B, n_r^B\\right)$;\n",
    "2. Generating the probability of each scenario;\n",
    "3. Generating the reward $r$ the scenario would result in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_CARS = 5\n",
    "\n",
    "# probs_customers[n_a_ordered, n_a_returned, n_b_ordered, n_b_returned]\n",
    "p2 = Poisson.get(2)\n",
    "p3 = Poisson.get(3)\n",
    "p4 = Poisson.get(4)\n",
    "probs_customers = np.zeros((MAX_NUM_CARS+1,) * 4)\n",
    "for n_a_ordered in range(MAX_NUM_CARS + 1):\n",
    "    for n_a_returned in range(MAX_NUM_CARS + 1):\n",
    "        for n_b_ordered in range(MAX_NUM_CARS + 1):\n",
    "            for n_b_returned in range(MAX_NUM_CARS + 1):\n",
    "                probs_customers[n_a_ordered, n_a_returned, n_b_ordered, n_b_returned] = p3[n_a_ordered] * p3[n_a_returned] * p4[n_b_ordered] * p4[n_b_returned]\n",
    "\n",
    "\n",
    "aidx_to_action = dict(enumerate(range(-5, 6, 1)))\n",
    "sidx_to_state = dict(enumerate([ (a, b) for a in range(0, MAX_NUM_CARS+1) for b in range(0, MAX_NUM_CARS+1)]) )\n",
    "state_to_sidx = {v: k for k,v in sidx_to_state.items()}\n",
    "\n",
    "n_states = len(state_to_sidx)\n",
    "n_actions = len(aidx_to_action)\n",
    "\n",
    "def get_n_scenarios():\n",
    "    return (MAX_NUM_CARS+1) ** 4\n",
    "\n",
    "def gen_get_scenarios():\n",
    "    index = 0\n",
    "    for n_o_A in range(0, MAX_NUM_CARS+1):\n",
    "        for n_r_A in range(0, MAX_NUM_CARS+1):\n",
    "            for n_o_B in range(0, MAX_NUM_CARS+1):\n",
    "                for n_r_B in range(0, MAX_NUM_CARS+1):\n",
    "                    yield (index, (n_o_A, n_r_A, n_o_B, n_r_B))\n",
    "                    index += 1\n",
    "\n",
    "\n",
    "def move_cars(s, a):\n",
    "    \"\"\"Move the cars. Does not validate!\"\"\"\n",
    "    return (s[0] - a, s[1] + a)\n",
    "\n",
    "def calc_new_state_and_reward(s, n_o_A, n_r_A, n_o_B, n_r_B):\n",
    "    \"\"\"Compute s' and r for this combination of state, action, and observed customer\n",
    "    behaviors.\n",
    "    \"\"\"\n",
    "    # Rent out as many cars as possible; accept returned cars\n",
    "    # Clip total number of cars at 20\n",
    "    n_rented_A = min([n_o_A, s[0]])\n",
    "    n_rented_B = min([n_o_B, s[1]])\n",
    "    sp = (\n",
    "        np.min([s[0] - n_rented_A + n_r_A, MAX_NUM_CARS]),\n",
    "        np.min([s[1] - n_rented_B + n_r_B, MAX_NUM_CARS]),\n",
    "        )\n",
    "    \n",
    "    # Calculate the reward\n",
    "    r = 10 * (n_rented_A + n_rented_B)\n",
    "    return sp, r\n",
    "\n",
    "\n",
    "e_r_sa = np.zeros((n_states, n_actions)) # trying to populate this\n",
    "n_scenarios = get_n_scenarios()\n",
    "for s_idx, state in sidx_to_state.items():\n",
    "    for a_idx, action in aidx_to_action.items():\n",
    "\n",
    "        # apply action\n",
    "        state_moved = move_cars(state, action)\n",
    "\n",
    "        # compute the expected reward for this state-action pair\n",
    "        R = -2 * np.abs(action) * np.ones(n_scenarios)\n",
    "        P = np.zeros(n_scenarios)\n",
    "        for scen_idx, scenario in gen_get_scenarios():\n",
    "            # get the probability of this scenario\n",
    "            p_scenario = probs_customers[scenario]\n",
    "\n",
    "            # get the reward and next state of this scenario\n",
    "            state_prime, rs = calc_new_state_and_reward(state_moved, *scenario)\n",
    "            # record probability or this scenario and the observed reward\n",
    "            R[scen_idx] += rs\n",
    "            P[scen_idx] = p_scenario\n",
    "\n",
    "        # calc expected reward\n",
    "        e_r_sa[s_idx, a_idx] = np.sum(R * P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidx = state_to_sidx[(2,2)]\n",
    "e_r_sa[sidx,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm computing reward for impossible state-action pairs: like $a=4$ for $s=\\left(2,2\\right)$. This should be OK, since any policy will have $\\pi\\left(a\\middle|s\\right) = 0$, thus this entry will be multiplied by zero when I compute the full sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The second term\n",
    "\n",
    "$$ \\mathbb{E}\\left[ v_k\\left(S_{t+1}\\right) \\middle| S_t = s, A_t = a\\right] = \\sum_{s'} v_k\\left(s'\\right) p\\left(s' \\middle| s, a\\right)$$\n",
    "\n",
    "Since this term depends on the current estimate $v_k$ of the state-value function, it can't be completely precomputed. However, we can compute the second distribution on the right, $p\\left(s'\\middle|s,a\\right)$. This will be an $S\\times S \\times A$ matrix. In the code, I will call it the transition probability matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [4:30:36<00:00, 36.82s/it]  \n"
     ]
    }
   ],
   "source": [
    "MAX_NUM_CARS = 20\n",
    "\n",
    "def get_scenario_probs():\n",
    "    # probs_customers[n_a_ordered, n_a_returned, n_b_ordered, n_b_returned]\n",
    "    p2 = Poisson.get(2)\n",
    "    p3 = Poisson.get(3)\n",
    "    p4 = Poisson.get(4)\n",
    "    probs_customers = np.zeros((MAX_NUM_CARS+1,) * 4)\n",
    "    for n_a_ordered in range(MAX_NUM_CARS + 1):\n",
    "        for n_a_returned in range(MAX_NUM_CARS + 1):\n",
    "            for n_b_ordered in range(MAX_NUM_CARS + 1):\n",
    "                for n_b_returned in range(MAX_NUM_CARS + 1):\n",
    "                    probs_customers[n_a_ordered, n_a_returned, n_b_ordered, n_b_returned] = p3[n_a_ordered] * p3[n_a_returned] * p4[n_b_ordered] * p2[n_b_returned]\n",
    "    return probs_customers\n",
    "\n",
    "\n",
    "aidx_to_action = dict(enumerate(range(-MAX_NUM_CARS, MAX_NUM_CARS+1, 1)))\n",
    "action_to_aidx = {v: k for k,v in aidx_to_action.items()}\n",
    "sidx_to_state = dict(enumerate([ (a, b) for a in range(0, MAX_NUM_CARS+1) for b in range(0, MAX_NUM_CARS+1) ]))\n",
    "state_to_sidx = {v: k for k,v in sidx_to_state.items()}\n",
    "\n",
    "n_states = len(state_to_sidx)\n",
    "n_actions = len(aidx_to_action)\n",
    "\n",
    "def get_n_scenarios():\n",
    "    return (MAX_NUM_CARS+1) ** 4\n",
    "\n",
    "def gen_get_scenarios():\n",
    "    index = 0\n",
    "    for n_o_A in range(0, MAX_NUM_CARS+1):\n",
    "        for n_r_A in range(0, MAX_NUM_CARS+1):\n",
    "            for n_o_B in range(0, MAX_NUM_CARS+1):\n",
    "                for n_r_B in range(0, MAX_NUM_CARS+1):\n",
    "                    yield (index, (n_o_A, n_r_A, n_o_B, n_r_B))\n",
    "                    index += 1\n",
    "\n",
    "def action_valid(s, a):\n",
    "    \"\"\"Check that action a is valid in state s\"\"\"\n",
    "    if a > 0:\n",
    "        return a <= s[0] # valid if there are enough cars at loc A\n",
    "    elif a < 0:\n",
    "        return -a <= s[1] # valid if there are enough cars at loc B\n",
    "    else:\n",
    "        return True # a == 0\n",
    "\n",
    "def move_cars(s, a):\n",
    "    \"\"\"Move the cars. Does not validate!\"\"\"\n",
    "    return (s[0] - a, s[1] + a)\n",
    "\n",
    "def calc_new_state_and_reward(s, n_o_A, n_r_A, n_o_B, n_r_B):\n",
    "    \"\"\"Compute s' and r for this combination of state, action, and observed customer\n",
    "    behaviors.\n",
    "    \"\"\"\n",
    "    # Rent out as many cars as possible; accept returned cars\n",
    "    # Clip total number of cars at 20\n",
    "    n_rented_A = min([n_o_A, s[0]])\n",
    "    n_rented_B = min([n_o_B, s[1]])\n",
    "    sp = (\n",
    "        np.min([s[0] - n_rented_A + n_r_A, MAX_NUM_CARS]),\n",
    "        np.min([s[1] - n_rented_B + n_r_B, MAX_NUM_CARS]),\n",
    "        )\n",
    "    # Calculate the reward\n",
    "    r = 10 * (n_rented_A + n_rented_B)\n",
    "    return sp, r\n",
    "\n",
    "\n",
    "def get_scenario_processor(state_moved):\n",
    "    def process_scenario(scen_idx, scenario):\n",
    "        # get the probability of this scenario\n",
    "        p_scenario = probs_customers[scenario]\n",
    "\n",
    "        # get the reward and next state of this scenario\n",
    "        state_prime, rs = calc_new_state_and_reward(state_moved, *scenario)\n",
    "\n",
    "        # get the transition probability for this scenario\n",
    "        sp_index = state_to_sidx[state_prime] # new state\n",
    "        # return stuff\n",
    "        return sp_index, rs, p_scenario\n",
    "\n",
    "    return process_scenario\n",
    "\n",
    "e_r_sa = np.zeros((n_states, n_actions)) # trying to populate this\n",
    "p_transitions = np.zeros((n_states, n_states, n_actions)) # trying to populate this\n",
    "# (new_state, old_state, action)\n",
    "\n",
    "n_scenarios = get_n_scenarios()\n",
    "for s_idx, state in tqdm(sidx_to_state.items()):\n",
    "    for a_idx, action in aidx_to_action.items():\n",
    "        if not action_valid(state, action):\n",
    "            e_r_sa[s_idx, a_idx] = 0\n",
    "            p_transitions[:, s_idx, a_idx] = 0\n",
    "            continue\n",
    "        else:\n",
    "            # apply action\n",
    "            state_moved = move_cars(state, action)\n",
    "\n",
    "            # compute the expected reward for this state-action pair\n",
    "            # apply parallel here\n",
    "            scenario_processor = get_scenario_processor(state_moved)\n",
    "            \n",
    "            results = Parallel(n_jobs=7)(delayed(scenario_processor)(scen[0], scen[1]) for scen in gen_get_scenarios())\n",
    "            \n",
    "            sp_indices = [r[0] for r in results] # s' (indices)\n",
    "            res_arr = np.array(results)[:,1:] # [reward, probability]\n",
    "            res_arr[:,1] += -2 * np.abs(action) # add moving cost\n",
    "\n",
    "            p_transitions[sp_indices, s_idx, a_idx] += res_arr[:,1] # add the probability that this happens\n",
    "            e_r_sa[s_idx, a_idx] = np.sum(res_arr[:,0] * res_arr[:,1]) # compute the expected reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        ...,\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        ...,\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        ...,\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        ...,\n",
       "        [  0.,   0., -36., ..., -36., -38., -40.],\n",
       "        [  0., -38., -36., ..., -36., -38.,   0.],\n",
       "        [-40., -38., -36., ..., -36.,   0.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        ...,\n",
       "        [  0.,   0., -36., ..., -36., -38., -40.],\n",
       "        [  0., -38., -36., ..., -36., -38., -40.],\n",
       "        [-40., -38., -36., ..., -36., -38.,   0.]],\n",
       "\n",
       "       [[  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,   0.,   0.,   0.],\n",
       "        ...,\n",
       "        [  0.,   0., -36., ..., -36., -38., -40.],\n",
       "        [  0., -38., -36., ..., -36., -38., -40.],\n",
       "        [-40., -38., -36., ..., -36., -38., -40.]]])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results # [(sp_index, rs, p_scenario)]\n",
    "sp_indices = [r[0] for r in results]\n",
    "\n",
    "results = np.array(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = (0,0,0,0)\n",
    "\n",
    "# get the probability of this scenario\n",
    "p_scenario = probs_customers[scenario]\n",
    "\n",
    "# get the reward and next state of this scenario\n",
    "state_moved = (1, )\n",
    "state_prime, rs = calc_new_state_and_reward(state_moved, *scenario)\n",
    "# # record probability of this scenario and the observed reward\n",
    "# # R[scen_idx] += rs\n",
    "# P[scen_idx] = p_scenario\n",
    "\n",
    "# # record the transition probability for this scenario\n",
    "# sp_index = state_to_sidx[state_prime] # new state\n",
    "# p_transitions[sp_index, s_idx, a_idx] += p_scenario\n",
    "# # return stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I parallelize this to speed things up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gen_get_scenarios())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    " v_{k+1}\\left(s\\right) & = \\sum_a \\pi\\left(a\\middle|s\\right) \\sum_{s',r} p\\left(s',r\\middle|s,a\\right) \\left(r + \\gamma v_{k}\\left(s'\\right) \\right) = \\sum_a \\pi\\left(a\\middle|s\\right) \\left[\\mathbb{E}\\left[ R_{t+1} \\middle| S_t=s, A_t=a \\right] + \\gamma \\mathbb{E}\\left[ v_k\\left(S_{t+1}\\right) \\middle| S_t = s, A_t = a\\right] \\right]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial policy: never move cars\n",
    "policy = np.zeros((n_actions, n_states))\n",
    "policy[action_to_aidx[0], :] = 1.0\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "v_old = np.zeros(len(sidx_to_state))\n",
    "v_new = np.zeros_like(v)\n",
    "for _ in range(5):\n",
    "    # calculate the expected value of subsequent states, under the current value estimate, and given (s,a)\n",
    "    for s_idx, state in sidx_to_state.items():\n",
    "        for a_idx, action in aidx_to_action.items():\n",
    "            v_new[s_idx] += policy[a_idx, s_idx] * (e_r_sa[s_idx, a_idx] + gamma * (v * p_transitions[:, s_idx, a_idx]).sum())\n",
    "\n",
    "    v_old = v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_transitions[:, s_idx, a_idx].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.flatten() * p_transitions[:, s_idx, a_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the state-value function of this policy\n",
    "v_0 = np.zeros(len(sidx_to_state)) # initial estimate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
