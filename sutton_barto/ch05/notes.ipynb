{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo methods don't require the knowledge of the complete dynamics of the system: they learn from experiencing the system. They can also use simulated experience (which itself requires us to be able to model the real environment, but *not* the entire dynamics of the system!).\n",
    "\n",
    "Only on the completion of an episode are the value estimates and policies updated.\n",
    "\n",
    "These methods use average sample returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With DP, we use our knowledge of the MDP to calculate value functions. Here, we use experience with the MDP to estimate value functions. The concepts of generalized policy iteration (GPI) are still used to optimize the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5.1 Monte Carlo Prediction\n",
    "\n",
    " Prediction is learning a state-value function $v_{\\pi}$ for a fixed policy $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of a state is the expected return (cumulative future discounted reward) starting from that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **first-visit MC method** estimates $v_{\\pi}$ as \n",
    "\\begin{align}\n",
    "v_{\\pi}\\left(s\\right) & = \\text{average of returns following first visits to } s \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We allow our agent to interact with the environment, acting under policy $\\pi$, for an episode. Record the episode as a trajectory of (state, action, reward) triples. Then we do a \"post-processing\" step:\n",
    "* Loop over every step of the episode and calculate the return (start at the last step and work backward);\n",
    "* For every first visit of state $s$, append $G$ for that first visit to a list of returns for that state.\n",
    "* For every state, set $V\\left(S_t\\right)$ to the average of $\\text{Returns}\\left(S_t\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
