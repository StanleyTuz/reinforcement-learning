{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-value functions\n",
    "Associated with any policy, $\\pi$, is a **state-value function**, written $v_{\\pi}$, which is a mapping $v_{\\pi}: \\mathcal{S}\\rightarrow \\mathbb{R}$. \n",
    "\n",
    "Informally, $v_{\\pi}\\left(s\\right)$ is the \"value\" of being in the state $s$. Formally, $v_{\\pi}$ is defined as $$ v_{\\pi}\\left(s\\right) := \\mathbb{E}\\left[ G_t \\, \\middle| \\, S_t = s\\right] $$ where $$ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots $$ is the total discounted reward observed after time $t$. Thus, the \"value\" of being in the state $s$ is defined as the expected total (discounted) reward after the agent has been in state $s$. Recall that $G_t$ is a random variable, being a linear combination of random variables, and so it may take on different values as any particular trajectory through the MDP, $$ \\left(S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, \\ldots\\right), $$ is just one observation of the agent in action. These random variables all depend, of course, on how the agent chooses to behave, i.e., its policy $\\pi$; the expectation is sometimes written as $\\mathbb{E}_{\\pi}$ to indicate this dependence.\n",
    "\n",
    "Thus, two different policies will generally see different expected total rewards for a given state, and so different value function.\n",
    "\n",
    "We can define a partial ordering on the space of value functions. From this ordering, we can extract a unique maximal value function, denoted $v_*$, and its associated (equivalent) optimal policy $\\pi_*$, so that $$v_*\\left(s\\right) := \\max_{\\pi} v_{\\pi}\\left(s\\right) \\forall s\\in \\mathcal{S}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action-value functions\n",
    "\n",
    "In a similar direction, we can ask how valuable it is to take a certain action when we are in a certain state. This defines the **action-value function** $$ q_{\\pi}\\left(s,a\\right) = \\mathbb{E}\\left[ G_t \\, \\middle| \\, S_t=s, A_t=a\\right].$$ Note that it doesn't make sense to ask about the value of an action without reference to a state, since the same action performed in different states will generally have different results. Again, take note that this function can only be defined with respect to a particular policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equation Derivations\n",
    "\n",
    "It's generally very difficult if not impossible to compute these expectation values for a given state or state-action pair by hand. Instead, we can derive a recursive relationship called the **Bellman equation** which a value function must satisfy. These are basically \"consistency conditions.\"\n",
    "\n",
    "The Bellman equation for the state-value function $v_{\\pi}$ is derived in section 3.5 in Sutton and Barto, and numbered equation 3.14. However, the manipulations they perform are not obvious, and so I'll derive these below.\n",
    "\n",
    "## Derivation for $v_{\\pi}$\n",
    "\n",
    "Given a state $s$, and from the definition of the state-value function, we have $$ \\begin{align} v_\\pi\\left(s\\right) & = \\mathbb{E}\\left[ G_t \\, \\middle| \\, S_t=s \\right] \\\\ & = \\mathbb{E}\\left[ R_{t+1} + \\gamma G_{t+1} \\, \\middle| \\, S_t=s \\right] \\\\ & = \\sum_{g,r} \\left(r + \\gamma g\\right) \\cdot p\\left(g,r\\, \\middle| \\, s\\right) \\end{align} $$ where for the last equality we just used the definition of (conditional) expectation, and where $g$ and $r$ range over the (finite, discrete) sets of possible total future rewards and possible next rewards, respectively. \n",
    "\n",
    "Next, we can use the law of total probability to bring more variables in; we want to bring in the possible actions we can take, denoted $a$, and the possible next states, $s'$, since these variables are involved in our MDP model. $$ \\begin{align} v_{\\pi}\\left(s\\right) & = \\sum_{g,r} \\left(r + \\gamma g\\right) \\cdot p\\left(g,r\\, \\middle| \\, s\\right)  \\\\ & = \\sum_{g,r,a,s'} \\left(r + \\gamma g\\right) \\cdot p\\left(g,r,a,s'\\, \\middle| \\, s\\right) \\end{align}$$ Next, let's use the definition of conditional probability to get the marginal over $g$: $$ \\begin{align} p\\left(g,r,a,s'\\, \\middle| \\, s\\right) & = p\\left(g\\, \\middle| \\, r,a,s',s\\right) p\\left(r,a,s'\\,\\middle|\\,s\\right) \\\\ & = p\\left(g\\, \\middle| \\, r,a,s',s\\right) p\\left(r,s'\\,\\middle|\\,s,a\\right) p\\left(a\\,\\middle|\\,s\\right) \\end{align} $$ By the Markov property of our MDP model, we can simplify the first factor on the right: $$ p\\left(g\\, \\middle| \\, r,a,s',s\\right) = p\\left(g\\,\\middle|\\,s'\\right).$$ The middle factor is just the dynamics of our MDP, and the last factor is the agent's policy, $$ p\\left(a\\,\\middle|\\,s\\right) = \\pi\\left(a\\,\\middle| \\,s\\right).$$ In the sum, we have  $$ \\begin{align} v_{\\pi}\\left(s\\right) & = \\sum_{g,r,a,s'} \\left(r + \\gamma g\\right) \\cdot p\\left(g \\, \\middle| \\, s'\\right) p\\left(r,s'\\,\\middle|\\,s,a\\right) \\pi\\left(a\\,\\middle|\\,s\\right) \\end{align} $$\n",
    "\n",
    "We can break up these sums over the two terms in $\\left(r+\\gamma g\\right)$, then simplify. The first term is $$ \\begin{align} \\sum_{g,r,a,s'} r \\cdot p\\left(g \\, \\middle| \\, s'\\right) p\\left(r,s'\\,\\middle|\\,s,a\\right) \\pi\\left(a\\,\\middle|\\,s\\right) & = \\sum_{r,a,s'} r \\cdot p\\left(r,s'\\,\\middle|\\,s,a\\right) \\pi\\left(a\\,\\middle|\\,s\\right) \\left(\\sum_g p\\left(g \\, \\middle| \\, s'\\right)\\right) \\\\ & = \\sum_{r,a,s'} r \\cdot p\\left(r,s'\\,\\middle|\\,s,a\\right) \\pi\\left(a\\,\\middle|\\,s\\right) \\end{align} $$ by the law of total probability.\n",
    "\n",
    "The second term is $$ \\begin{align} \\sum_{g,r,a,s'} g \\cdot p\\left(g \\, \\middle| \\, s'\\right) p\\left(r,s'\\,\\middle|\\,s,a\\right) \\pi\\left(a\\,\\middle|\\,s\\right) & = \\sum_{r,a,s'} p\\left(r,s'\\,\\middle|\\,s,a\\right) \\pi\\left(a\\,\\middle|\\,s\\right) \\left(\\sum_g g \\cdot p\\left(g \\, \\middle| \\, s'\\right)\\right) \\\\ & = \\sum_{r,a,s'} p\\left(r,s'\\,\\middle|\\,s,a\\right) \\pi\\left(a\\,\\middle|\\,s\\right) \\mathbb{E}\\left[ G_{t+1} \\, \\middle| \\, S_{t+1}=s'\\right] \\\\ & = \\sum_{r,a,s'} p\\left(r,s'\\,\\middle|\\,s,a\\right) \\pi\\left(a\\,\\middle|\\,s\\right) v_{\\pi}\\left(s'\\right) \\\\ \\end{align} $$ where the second equality comes from the definition of expectation value, and the last comes from the definition of state-value function.\n",
    "\n",
    "Taken together, the two terms give us $$ \\begin{align} v_{\\pi}\\left(s\\right) & = \\sum_{r,a,s'} \\left(r+ \\gamma v_{\\pi}\\left(s'\\right)\\right) \\pi\\left(a\\,\\middle|\\,s\\right)p\\left(r,s'\\,\\middle|\\,s,a\\right) \\\\ & = \\sum_a \\pi\\left(a\\,\\middle|\\,s\\right) \\sum_{r,s'} \\left(r+\\gamma v_{\\pi}\\left(s'\\right) \\right) p\\left(r,s'\\,\\middle|\\,s,a\\right) \\\\ & = \\sum_a \\pi\\left(a\\,\\middle|\\,s\\right) \\sum_{s'} \\sum_{r} p\\left(s',r\\,\\middle|\\,s,a\\right) \\left[r+\\gamma v_{\\pi}\\left(s'\\right) \\right] \\end{align} $$ for all $s \\in \\mathcal{S}$, and this is exactly equation 3.14 in the text.\n",
    "\n",
    "Note that in David Silver's RL lectures, he presents these equations slightly differently. The equations are actually the same, but he introduces the expressions $$ \\mathcal{P}_{ss'}^{a} = p\\left(S_{t+1} = s'\\,\\middle|\\, S_t = s, A_t=a\\right) $$ for the transition probabilities and $$ \\mathcal{R}_s^a = \\mathbb{E}\\left[ R_{t+1} \\, \\middle| \\, S_t=s, A_t=a \\right] $$ for the expected reward when taking action $a$ in state $s$. Only some more minor algebraic manipulations are needed to derive his equations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation for $q_{\\pi}$\n",
    "\n",
    "This derivation is very similar. The main conceptual difference is that since we want our final result to have an expectation conditioned on both the next state, $s'$, and the next action, $a'$, we need to pull both $s'$ and $a'$ into our work via the law of total probability. Let's start.\n",
    "\n",
    "$$ \\begin{align} q_{\\pi}\\left(s,a\\right) & = \\mathbb{E}\\left[ G_t \\,\\middle|\\, S_t=s, A_t = a \\right] \\\\ & = \\mathbb{E}\\left[ R_{t+1} + \\gamma G_{t+1} \\, \\middle| \\, S_t=s, A_t=a \\right] \\\\ & = \\sum_{g,r} \\left(r + \\gamma g\\right) p\\left(g,r \\, \\middle|\\, s, a\\right) \\\\ & = \\sum_{g,r,a',s'} \\left(r + \\gamma g\\right) p\\left(g,r,a',s' \\, \\middle|\\, s, a\\right) \\\\ & = \\sum_{g,r,a',s'} \\left(r+\\gamma g\\right) p\\left(g\\,\\middle|\\,r,a',s',s,a\\right) p\\left(a'\\,\\middle|\\,r,s',s,a\\right)p\\left(r,s'\\,\\middle|\\,s,a\\right) \\\\ & = \\sum_{g,r,a',s'} \\left(r+\\gamma g\\right) p\\left(g\\,\\middle|\\,a',s'\\right) \\pi\\left(a'\\,\\middle|\\,s'\\right)p\\left(r,s'\\,\\middle|\\,s,a\\right) \\\\ & = \\sum_{a'} \\pi\\left(a'\\,\\middle|\\,s'\\right) \\sum_{s'}\\sum_r p\\left(r,s'\\,\\middle|\\,s,a\\right)\\left[r+\\gamma q_{\\pi}\\left(s',a'\\right)\\right] \\\\\\end{align} $$\n",
    "\n",
    "FIX THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld example\n",
    "\n",
    "For every state-action pair, need to select a reward (stochastic or deterministic) and a result state (observation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "# random policy\n",
    "policy = np.ones((5*5, 4)) / 4 # N S E W\n",
    "\n",
    "# action spaces: mapping of state to possible actions\n",
    "# in this example, all actions are possible\n",
    "action_spaces = { i: (0,1,2,3) for i in range(5*5) }\n",
    "\n",
    "# dynamics: map a state-action pair to lists of state'-reward-probability triples.\n",
    "# for each state-action pair, the probs in these lists must sum to 1.\n",
    "# In this example, the dynamics are deterministic (degenerate distributions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_srp(state_action_pairs):\n",
    "    \"\"\"Assign state'-reward-prob triples for state-action pairs.\n",
    "        Helper function.\n",
    "    \"\"\"\n",
    "    dynamics = dict()\n",
    "    for sap in state_action_pairs:\n",
    "        if sap[0] < 5 and sap[1] == 0: # top row, moving up\n",
    "            dynamics[sap] = [(sap[0], -1.0, 1.0)]\n",
    "        elif sap[0] > 19 and sap[1] == 1: # bottom row, moving down\n",
    "            dynamics[sap] = [(sap[0], -1.0, 1.0)]\n",
    "        elif (sap[0] + 1) % 5 == 0 and sap[1] == 2: # right col, moving east\n",
    "            dynamics[sap] = [(sap[0], -1.0, 1.0)]\n",
    "        elif (sap[0] % 5) == 0 and sap[1] == 3: # left col, moving west\n",
    "            dynamics[sap] = [(sap[0], -1.0, 1.0)]\n",
    "        else:\n",
    "            if sap[1] == 0:   # move north\n",
    "                dynamics[sap] = [(sap[0]-5, 0.0, 1.0)]\n",
    "            elif sap[1] == 1: # move south\n",
    "                dynamics[sap] = [(sap[0]+5, 0.0, 1.0)]\n",
    "            elif sap[1] == 2: # move east\n",
    "                dynamics[sap] = [(sap[0]+1, 0.0, 1.0)]\n",
    "            else:             # move west\n",
    "                dynamics[sap] = [(sap[0]-1, 0.0, 1.0)]\n",
    "\n",
    "    # special states\n",
    "    dynamics[(1, 0)] = [(21, 10.0, 1.0)]\n",
    "    dynamics[(1, 1)] = [(21, 10.0, 1.0)]\n",
    "    dynamics[(1, 2)] = [(21, 10.0, 1.0)]\n",
    "    dynamics[(1, 3)] = [(21, 10.0, 1.0)]\n",
    "    dynamics[(3, 0)] = [(13,  5.0, 1.0)]\n",
    "    dynamics[(3, 1)] = [(13,  5.0, 1.0)]\n",
    "    dynamics[(3, 2)] = [(13,  5.0, 1.0)]\n",
    "    dynamics[(3, 3)] = [(13,  5.0, 1.0)]\n",
    "                \n",
    "    return dynamics\n",
    "\n",
    "state_action_pairs = [(s,a) for s in range(5*5) for a in range(4)]\n",
    "dynamics = set_srp(state_action_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking action 1 in state 0...\n",
      "    ...observed state 5 and reward 0.0.\n",
      "taking action 3 in state 5...\n",
      "    ...observed state 5 and reward -1.0.\n",
      "taking action 2 in state 5...\n",
      "    ...observed state 6 and reward 0.0.\n",
      "taking action 0 in state 6...\n",
      "    ...observed state 1 and reward 0.0.\n",
      "taking action 2 in state 1...\n",
      "    ...observed state 21 and reward 10.0.\n",
      "taking action 0 in state 21...\n",
      "    ...observed state 16 and reward 0.0.\n",
      "taking action 3 in state 16...\n",
      "    ...observed state 15 and reward 0.0.\n",
      "taking action 1 in state 15...\n",
      "    ...observed state 20 and reward 0.0.\n",
      "taking action 0 in state 20...\n",
      "    ...observed state 15 and reward 0.0.\n",
      "taking action 2 in state 15...\n",
      "    ...observed state 16 and reward 0.0.\n"
     ]
    }
   ],
   "source": [
    "# simulate a walk through the environment\n",
    "state = 0\n",
    "\n",
    "def select_action(state):\n",
    "    # use policy to choose action\n",
    "    return np.random.choice(range(4), p=policy[state])\n",
    "\n",
    "def step(state, action):\n",
    "    # use dynamics to get env's response to action\n",
    "    probs = [p for _,_,p in dynamics[(state, action)]]\n",
    "    n_possible = len(probs)\n",
    "    # print(probs)\n",
    "    sel = np.random.choice(range(n_possible), p=probs) # select it stochastically\n",
    "    return dynamics[(state, action)][sel][0:2]\n",
    "\n",
    "\n",
    "\n",
    "n_iters = 10\n",
    "actions = np.zeros(n_iters, dtype=int)\n",
    "states = np.zeros(n_iters+1, dtype=int) # start in state 0\n",
    "rewards = np.zeros(n_iters)\n",
    "\n",
    "\n",
    "for n in range(n_iters):\n",
    "    actions[n] = select_action(state)\n",
    "    print(f'taking action {actions[n]:d} in state {states[n]:d}...')\n",
    "    states[n+1], rewards[n] = step(states[n], actions[n])\n",
    "    print(f'    ...observed state {states[n+1]:d} and reward {rewards[n]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -1., -1., -1.,  9.,  9.,  9.,  9.,  9.,  9.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the value function $v_{\\pi}$ for this policy. This example is relatively simple in that it's a finite MDP, there is a small cardinality action space for every state, the policy is uniformly random, and the dynamics (subsequent state and reward) are deterministic, i.e., the dynamics distribution assigns non-zero probability to only one combination of $s'$ and $r$. Thus, for every state $s$, we have an equation $$ v_{\\pi}\\left(s\\right) = \\sum_a \\pi\\left(a\\,\\middle|\\,s\\right) \\sum_{s'} \\sum_{r} p\\left(s',r\\,\\middle|\\,s,a\\right) \\left[r+\\gamma v_{\\pi}\\left(s'\\right) \\right] $$ where the innermost two sums degenerate to a single term. For example, $$ v_{\\pi}\\left(s\\right) = \\sum_a \\frac{1}{4}\\left[ r_s^a + \\gamma v_{\\pi}\\left(\\left(s'\\right)_s^a\\right) \\right] $$ where $r_s^a$ is the reward obtained by taking action $a$ in state $s$, and $\\left(s'\\right)_s^a$ is the state obtained by taking action $a$ in state $s$. Distributing the sum, we get $$ v_{\\pi}\\left(s\\right) = \\frac{1}{4}\\sum_a r_s^a + \\frac{\\gamma}{4}\\sum_a v_{\\pi}\\left(\\left(s'\\right)_s^a\\right). $$ This can be interpreted by saying that the value of a state is the average (immediate) reward obtained by taking an action plus the average of the values of states where we might end up.\n",
    "\n",
    "We number the states as \n",
    "||||||\n",
    "| - | - | - | - | - |\n",
    "| 0 | 1 | 2 | 3 | 4 |\n",
    "| 5 | 6 | 7 | 8 | 9 |\n",
    "| 10 | 11 | 12 | 13 | 14 |\n",
    "| 15 | 16 | 17 | 18 | 19 | \n",
    "| 20 | 21 | 22 | 24 | 24 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3.3      8.8      4.4      5.3      1.5\n",
      "   1.5      3.0      2.3      1.9      0.5\n",
      "   0.1      0.7      0.7      0.4     -0.4\n",
      "  -1.0     -0.4     -0.4     -0.6     -1.2\n",
      "  -1.9     -1.3     -1.2     -1.4     -2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.zeros((5*5, 5*5))\n",
    "b = np.zeros((5*5,1))\n",
    "for s_idx in range(5*5):\n",
    "    A[s_idx, s_idx] = 1.\n",
    "    # calc average reward\n",
    "    r_bar = 0\n",
    "    for a_idx in range(4):\n",
    "        r_bar += dynamics[(s_idx, a_idx)][0][1]\n",
    "    r_bar /= 4\n",
    "    b[s_idx] = r_bar\n",
    "\n",
    "    # record coefficients for other terms\n",
    "    for a_idx in range(4):\n",
    "        result_state = dynamics[(s_idx, a_idx)][0][0]\n",
    "        A[s_idx, result_state] -= gamma / 4\n",
    "\n",
    "value = np.linalg.solve(A, b).reshape(5,5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'{value[i,0]:6.1f}   {value[i,1]:6.1f}   {value[i,2]:6.1f}   {value[i,3]:6.1f}   {value[i,4]:6.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand that this is the value function, $v_{\\pi}$, for the random policy, $\\pi$. For a given state, the value should be interpreted as the expected total (discounted) reward if we start at that given state and proceed onward, behaving according to the policy. Note that we didn't compute this value function by actually taking the expectation value of infinitely many trajectories through the MDP; rather, we used the Bellman equation to relate the value at a state with the value of subsequent states, and since it's a finite problem this resulted in a finite linear system which we solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Futher questions\n",
    "\n",
    "- These recursive expressions are all exact when we are dealing with a finite MDP, i.e., where all the sets we're summing over are finite (thus discrete). What about cases, for example, with continuous rewards?\n",
    "- Is there an easier way to derive the Bellman equations, for example, by using the \"law of iterated expectation\"?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self):\n",
    "        self._state = 0\n",
    "        # self._dynamics = 0\n",
    "\n",
    "    def step(self, action: int):\n",
    "        \"\"\"Take an action. Return new obs, reward.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self._policy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
