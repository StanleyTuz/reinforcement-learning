{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Policies and Value Functions\n",
    "\n",
    "I will implement Example 3.5: Gridworld in which the authors design a grid environment on which an agent moves and receives rewards. The state space is a set of 25 tiles, and the action space is a movement up, down, left, or right. In this example we fix the agent's policy to the equiprobable random policy $\\pi$ and we wish to compute the state-value function for this policy.\n",
    "\n",
    "The state-value function can be computed exactly in this case by solving the Bellman equation as a system of linear equations, since this is a finite MDP problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_gridworld_dynamics():\n",
    "    \"\"\"This function constructs a list-of-tuples representation for the environment dynamics.\n",
    "    We could also have considered a pd.DataFrame version which would have clarity through\n",
    "    column names. Or perhaps a list-of-namedtuples. (TRY THIS)\n",
    "\n",
    "    Returns (s, a, s', r, p)\n",
    "\n",
    "    This returns the list of p(s', r | s, a).\n",
    "    \"\"\"\n",
    "    dynamics = []\n",
    "    # s, a, s', r, p\n",
    "\n",
    "    # Move up\n",
    "    up = 0\n",
    "    for s in range(0, 5): # top row\n",
    "        dynamics.append((s, up, s, -1, 1.0))\n",
    "    for s in range(5, 25): # all rows except top row\n",
    "        dynamics.append((s, up, s-5, 0, 1.0))\n",
    "\n",
    "    # Move down\n",
    "    down = 1\n",
    "    for s in range(20, 25): # bottom row\n",
    "        dynamics.append((s, down, s, -1, 1.0))\n",
    "    for s in range(0, 20): # all rows except bottom row\n",
    "        dynamics.append((s, down, s+5, 0, 1.0))\n",
    "\n",
    "    # Move left\n",
    "    left = 2\n",
    "    for s in range(0, 25): # all squares\n",
    "        if s % 5 == 0: # left column\n",
    "            dynamics.append((s, left, s, -1, 1.0))\n",
    "        else:\n",
    "            dynamics.append((s, left, s-1, 0, 1.0))\n",
    "\n",
    "    # Move right\n",
    "    right = 3\n",
    "    for s in range(0, 25): # all squares\n",
    "        if (s+1) % 5 == 0: # right column\n",
    "            dynamics.append((s, right, s, -1, 1.0))\n",
    "        else:\n",
    "            dynamics.append((s, right,s+1, 0, 1.0))\n",
    "\n",
    "\n",
    "    # Add in the arbitrary dynamics\n",
    "    for tup in dynamics:\n",
    "        if tup[0] == 1 or tup[0] == 3:\n",
    "            dynamics.remove(tup)\n",
    "\n",
    "    _ = [dynamics.append((1, a, 21, 10, 1.0)) for a in range(4)]\n",
    "    _ = [dynamics.append((3, a, 13, 5, 1.0)) for a in range(4)]\n",
    "\n",
    "    return dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_er_policy():\n",
    "    \"\"\"Constructs a list-of-tuples representation of the equiprobable random policy.\n",
    "    \n",
    "    (s, a, p)\n",
    "    \"\"\"\n",
    "    return [(s, a, 1/4) for a in range(4) for s in range(25)]\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, gamma: float = 1.0):\n",
    "        self.pi_ = construct_er_policy()\n",
    "        self.gamma = gamma\n",
    "        self.total_return = 0.0\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_return = 0.0\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        \"\"\"obs = observed state\"\"\"\n",
    "        # Get possible actions to take from this state\n",
    "        actions_ = [x for x in self.pi_ if x[0] == obs]\n",
    "        probs = [x[-1] for x in actions_]\n",
    "        act_idx = np.random.choice(range(len(probs)), p=probs)\n",
    "        return actions_[act_idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import Any, Tuple\n",
    "\n",
    "class Env(ABC):\n",
    "    pass\n",
    "\n",
    "class State():\n",
    "    pass\n",
    "\n",
    "class Gridworld(Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = {0: \"Up\", 1: \"Down\", 2: \"Left\", 3: \"Right\"}\n",
    "        self.state = np.random.randint(0, 25)\n",
    "        self.dynamics = construct_gridworld_dynamics()\n",
    "\n",
    "    def step(self, action: int) -> Tuple[State, float, bool, Any]:\n",
    "        \"\"\"Take an action and run the dynamics of the MDP.\n",
    "        \n",
    "        Returns:\n",
    "            State: Resultant state of the environment after taking the action. (\"Observation\")\n",
    "            float: Reward obtained by taking the action.\n",
    "            bool: Indicates termination of the episode.\n",
    "            Any: Additional information.\n",
    "        \"\"\"\n",
    "        # See what possible result states and rewards can result from performing this action\n",
    "        # in the current state\n",
    "        state = self.state\n",
    "        futures_ = [x for x in self.dynamics if x[0] == state and x[1] == action]\n",
    "        # print(futures_)\n",
    "\n",
    "        # Randomly select from these\n",
    "        probs = [x[-1] for x in futures_]\n",
    "        future_idx = np.random.choice(range(len(probs)), p=probs)\n",
    "        result = futures_[future_idx]\n",
    "\n",
    "        # Update the state\n",
    "        self.state = result[2]\n",
    "        # Return results\n",
    "        return (self.state, result[3], False, None)\n",
    "\n",
    "    def state_to_coord(self):\n",
    "        y_coord = 5 - ( self.state // 5) - 1\n",
    "        x_coord = self.state % 5\n",
    "        return x_coord, y_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will generally need to compute expectation values, I need to store the problem dynamics as probability distributions somehow. Recall that the dynamics is the function $$ p\\left(s', r \\middle| s, a\\right): \\mathcal{S}\\times \\mathcal{R} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\left[0,1\\right] $$ subject to the obvious normalization conditions. This will generally be a 4-dimensional array.\n",
    "\n",
    "For this particular problem, we have $\\left|\\mathcal{S}\\right| = 25$, $\\left|\\mathcal{A}\\right| = 4$, and $\\left|\\mathcal{R}\\right| = 5$, so $p$ will be an array with $25^2 \\times 4 \\times 4 = 10,000$ entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead I implemented it as a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.state = 10\n",
      "Taking step down...\n",
      " Got reward  0\n",
      "env.state = 15\n",
      "Taking step down...\n",
      " Got reward  0\n",
      "env.state = 20\n",
      "Taking step down...\n",
      " Got reward -1\n",
      "env.state = 20\n",
      "Taking step down...\n",
      " Got reward -1\n"
     ]
    }
   ],
   "source": [
    "env = Gridworld()\n",
    "for _ in range(4):\n",
    "    print(f\"env.state = {env.state}\")\n",
    "    print(\"Taking step down...\")\n",
    "    res = env.step(1)\n",
    "    print(f\" Got reward {res[1]:2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set(xlim=[-0.5,4.5], ylim=[-0.5,4.5])\n",
    "ax.set_xticks(np.arange(-0.5, 4.5, 1))\n",
    "ax.set_yticks(np.arange(-0.5, 4.5, 1))\n",
    "ax.xaxis.set_ticklabels([])\n",
    "ax.yaxis.set_ticklabels([])\n",
    "ax.grid()\n",
    "\n",
    "env = Gridworld()\n",
    "env.state = 14\n",
    "x_, y_ = env.state_to_coord()\n",
    "ax.scatter(x_,y_, marker='o', s=100, color='r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to solve the Bellman equation --- a system of linear equations --- to find the value function for this equiprobable random policy, $\\pi$. There are $\\left|\\mathcal{S}\\right|=25$ states, and each state has a corresponding equation, so this is a system of 25 equations in 25 unknowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_ = construct_er_policy() # s, a, p\n",
    "dynamics = construct_gridworld_dynamics() # s, a, s', r, p\n",
    "\n",
    "A = np.zeros((25, 25))\n",
    "b = np.zeros((25, 1))\n",
    "\n",
    "states = range(25)\n",
    "actions = range(4)\n",
    "rewards = [-1, 0, 10, 5]\n",
    "for s in states:\n",
    "    # Create the constant term\n",
    "    constant_term = 0.0\n",
    "    # Get probs for actions we can take in this state\n",
    "    pol_s = [x for x in pi_ if x[0] == s]\n",
    "    \n",
    "    # policy has the actions with nonzero probability, so we can sum over this\n",
    "    for _, a, p_a in pol_s:\n",
    "        dyn_sa = [x for x in dynamics if x[0] == s and x[1] == a]\n",
    "        for _, _, sp, r, p_spr in dyn_sa:\n",
    "            constant_term += p_a * p_spr * r\n",
    "\n",
    "    b[s] = -constant_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.5 ],\n",
       "       [-10.  ],\n",
       "       [  0.25],\n",
       "       [ -5.  ],\n",
       "       [  0.5 ],\n",
       "       [  0.25],\n",
       "       [ -0.  ],\n",
       "       [ -0.  ],\n",
       "       [ -0.  ],\n",
       "       [  0.25],\n",
       "       [  0.25],\n",
       "       [ -0.  ],\n",
       "       [ -0.  ],\n",
       "       [ -0.  ],\n",
       "       [  0.25],\n",
       "       [  0.25],\n",
       "       [ -0.  ],\n",
       "       [ -0.  ],\n",
       "       [ -0.  ],\n",
       "       [  0.25],\n",
       "       [  0.5 ],\n",
       "       [  0.25],\n",
       "       [  0.25],\n",
       "       [  0.25],\n",
       "       [  0.5 ]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, 3, 0.25)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in pi_ if x[0:2] == (s, a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = range(25)\n",
    "actions = range(4)\n",
    "rewards = [-1, 0, 10, 5]\n",
    "S = np.zeros((25, 25))\n",
    "gamma = 0.9\n",
    "\n",
    "for s in states:\n",
    "    for sp in states:\n",
    "        accum = 0.0\n",
    "        for a in actions: # sum_a\n",
    "            # get prob of taking this action in this state\n",
    "            _,_, p_a_s = [x for x in pi_ if x[0:2] == (s, a)][0]\n",
    "            # Get possible resultant dynamics from taking this action in this state\n",
    "            # prob of reward, s' given s, a\n",
    "            # Note that the dynamics here are deterministic, so this is more trivial than I'm \n",
    "            # making it.\n",
    "            p_rsp_sa_list = [x for x in dynamics if x[0:3] == (s, a, sp)]\n",
    "\n",
    "            for res in p_rsp_sa_list: # sum_r\n",
    "                accum += p_a_s * res[4]\n",
    "        S[s, sp] = gamma * accum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.zeros((25, 1))\n",
    "for s in states:\n",
    "    accum = 0.0\n",
    "    for a in actions: # sum_a\n",
    "        _, _, p_sa = [ x for x in pi_ if x[0:2] == (s, a) ][0]\n",
    "        # print(p_sa)\n",
    "        for sp in states: # sum_sp\n",
    "            p_rsp_sa_list = [x for x in dynamics if x[0:3] == (s, a, sp)]\n",
    "            # print(p_rsp_sa_list)\n",
    "            for res in p_rsp_sa_list:\n",
    "                accum += res[3] * p_sa * res[4]\n",
    "    B[s] = accum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.eye(25) - S\n",
    "b = B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.linalg.inv(A) @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_square = v.reshape(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set(xlim=[-0.5,4.5], ylim=[-0.5,4.5])\n",
    "ax.set_xticks(np.arange(-0.5, 4.5, 1))\n",
    "ax.set_yticks(np.arange(-0.5, 4.5, 1))\n",
    "ax.xaxis.set_ticklabels([])\n",
    "ax.yaxis.set_ticklabels([])\n",
    "ax.grid()\n",
    "\n",
    "def state_to_txt_coord(s: int):\n",
    "    y_coord = 5 - ( s // 5) - 1\n",
    "    x_coord = s % 5\n",
    "    return x_coord, y_coord\n",
    "\n",
    "for s in states:\n",
    "    x_t, y_t = state_to_txt_coord(s)\n",
    "    ax.text(x=x_t, y=y_t, s=f\"{v[s][0]:.1f}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5ac7c6156dd1d106c0eb61ce42a0bdd7cc19128b6e13749ed889b170c5c7a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
