{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to solve Jack's Car Rental example from Sutton & Barto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a finite MDP problem, I would like to be able to store all of the probability distributions in arrays in memory. However, since there are $20^2 = 400$ states, $41$ possible actions, and an untold number $\\left|\\mathcal{R}\\right|$ of rewards, the size of this array will be quite large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference between this problem and something like the previous Gridworld examples is that the dynamics here are stochastic; the reward, $r$, obtained for taking an action in a particular state as well as the resultant state, $s'$, both depend on the number of customers that walk into the two locations to rent or to return cars.\n",
    "\n",
    "The dynamics for the problem is\n",
    "\\begin{align}\n",
    "    p\\left(s', r\\, \\middle| \\, s, a\\right) & = \\sum_{n_1, n_2, m_1, m_2} p\\left(s', r \\, \\middle| \\, s, a, n_1, n_2, m_1, m_2 \\right) p\\left(n_1\\right)p\\left(n_2\\right)p\\left(m_1\\right)p\\left(m_2\\right) \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $n_1$ and $n_2$ are the numbers of customers that come to rent a car at location 1 and 2, respectively, and $m_1$ and $m_2$ are the numbers of customers that come to return cars. The distributions of these random variables are assumed to be Poisson distributions: this is the basis of our known model of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about that first distribution with all of the conditioning variables? Think about it: if we know the values of all of those variables, we can deterministically compute the next state and the reward obtained!\n",
    "\n",
    "$$ r = 10 \\cdot \\min \\left\\{ n_1, s_1 - a \\right\\} + 10 \\cdot \\left\\{ n_2, s_2 + a\\right\\} - 2 \\cdot a $$\n",
    "\n",
    "$$ \\left[ \\begin{array}{c} s_1' \\\\ s_2' \\end{array}\\right] = \\left[ \\begin{array}{cc} \\max \\left\\{ s_1 - a - n_1, \\, 0 \\right\\} + m_1 \\\\ \\max \\left\\{ s_2 + a - n_2, \\, 0 \\right\\} + m_2 \\end{array}\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson as poi\n",
    "\n",
    "x = range(0, 10)\n",
    "\n",
    "n1_dist = poi.pmf(k=x, mu=3)\n",
    "n1_dist /= n1_dist.sum()\n",
    "\n",
    "n2_dist = poi.pmf(k=x, mu=4)\n",
    "n2_dist /= n2_dist.sum()\n",
    "\n",
    "m1_dist = poi.pmf(k=x, mu=3)\n",
    "m1_dist /= m1_dist.sum()\n",
    "\n",
    "m2_dist = poi.pmf(k=x, mu=2)\n",
    "m2_dist /= m2_dist.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = dict(enumerate([(i,j) for i in range(21) for j in range(21)]))\n",
    "def state_tup_to_idx(state_tup, states=states):\n",
    "    \n",
    "    for k, tup in states.items():\n",
    "        if state_tup == tup:\n",
    "            return k\n",
    "    raise ValueError(f\"Tuple {state_tup} not found in states!\")\n",
    "\n",
    "\n",
    "actions = dict(enumerate(range(-5, 6)))\n",
    "def action_to_idx(action, actions=actions):\n",
    "    for k, act in actions.items():\n",
    "        if act == action:\n",
    "            return k\n",
    "    raise ValueError(f\"Action {action} not found in actions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_r(s_idx, a_idx, n_1, n_2):\n",
    "    s_1, s_2 = states[s_idx]\n",
    "    a_val = actions[a_idx]\n",
    "    return int(10 * np.min([n_1, s_1 - a_val]) + 10 * np.min([n_2, s_2 + a_val]) - 2 * a_val)\n",
    "\n",
    "\n",
    "def calc_sp_idx(s_idx, a_idx, n_1, n_2, m_1, m_2) -> int:\n",
    "    s_1, s_2 = states[s_idx]\n",
    "    a_val = actions[a_idx]\n",
    "    sp_tup = (\n",
    "        np.min(\n",
    "            [np.max([s_1 - a_val - n_1, 0]) + m_1, 20]\n",
    "        ), \n",
    "        np.min(\n",
    "            [np.max([s_2 + a_val - n_2, 0]) + m_2, 20]\n",
    "        )\n",
    "    )\n",
    "    return state_tup_to_idx(sp_tup)\n",
    "\n",
    "def calc_p(n_1, n_2, m_1, m_2):\n",
    "    return np.exp(np.log([n1_dist[n_1], n2_dist[n_2], m1_dist[m_1], m2_dist[m_2]]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamics(s_idx: int, a_idx: int):\n",
    "    dynamics_list = []\n",
    "    for n_1 in range(len(n1_dist)):\n",
    "        for n_2 in range(len(n2_dist)):\n",
    "            for m_1 in range(len(m1_dist)):\n",
    "                for m_2 in range(len(m2_dist)):\n",
    "                    # calc reward\n",
    "                    r = calc_r(s_idx, a_idx, n_1, n_2)\n",
    "                    # calc s'\n",
    "                    sp_idx = calc_sp_idx(s_idx, a_idx, n_1, n_2, m_1, m_2)\n",
    "                    # calc (lookup) prob\n",
    "                    p = calc_p(n_1, n_2, m_1, m_2)\n",
    "                    dynamics_list.append((sp_idx, r, p))\n",
    "    return dynamics_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_update_for_v(s_idx: int, v: np.ndarray, policy: np.ndarray, gamma: float):\n",
    "    n_actions = len(actions)\n",
    "\n",
    "    expected_reward = 0.0\n",
    "    for a_idx in actions:\n",
    "        dl = dynamics(s_idx, a_idx)\n",
    "        for sp, r, p in dl:\n",
    "            expected_reward += policy[sp, a_idx] * p * (r + gamma * v[sp])\n",
    "    return expected_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.zeros(len(states))\n",
    "policy = np.ones((len(states), len(actions))) / len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do one step of iterative policy evaluation. This constructs an approximation $v_1 \\approx v_{\\pi}$ which should improve upon our initial guess $v_0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_new = np.zeros_like(v)\n",
    "for s_idx in states:\n",
    "    v_new[s_idx] = calc_update_for_v(s_idx, v, policy, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7fabca6e29afdb3bac51c9dd85c4930090a61a50da18e87b68da8b940a526be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
